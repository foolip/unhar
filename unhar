#!/usr/bin/env python3
#
# Unpacks a raw httparchive har file into one file per resource.

import base64
import gzip
import json
import os
import sys

from urllib.parse import urlparse

def unhar(path):
    openfunc = gzip.open if path.endswith('.gz') else open
    with openfunc(path, mode='rt', encoding='utf-8', errors='ignore') as f:
        data = json.load(f, strict=False)

    for entry in data['log']['entries']:
        url = urlparse(entry['request']['url'])

        content = entry['response']['content']['text']
        encoding = entry['response']['content']['encoding']
        if encoding == 'base64':
            content = base64.b64decode(content)
        else:
            content = content.encode('utf-8')

        outpath = os.path.join(url.hostname, url.path[1:])

        if os.path.exists(outpath):
            print(f'{outpath} already exists, not overwriting')
            continue

        print(f'Extracting {outpath}')
        os.makedirs(os.path.dirname(outpath), exist_ok=True)
        with open(outpath, mode='wb') as f:
            f.write(content)

if __name__ == '__main__':
    for path in sys.argv[1:]:
        try:
            unhar(path)
        except:
            print('Failed to extract ' + path)
